"""3-stage LLM Council orchestration."""

from typing import List, Dict, Any, Tuple, Optional
from .providers import query_models_parallel, query_model, get_provider
from .config import COUNCIL_MODELS, CHAIRMAN_MODEL

# Dynamic configuration (can be overridden per request)
_dynamic_config: Optional[Dict[str, Any]] = None


def set_council_config(models: List[str], chairman: str):
    """
    Set dynamic council configuration.
    
    Args:
        models: List of model names to use
        chairman: Chairman model name
    """
    global _dynamic_config
    _dynamic_config = {
        "models": models,
        "chairman": chairman
    }


def get_council_models() -> List[str]:
    """Get the list of council models (dynamic or default)."""
    if _dynamic_config:
        return _dynamic_config["models"]
    return COUNCIL_MODELS


def get_chairman_model() -> str:
    """Get the chairman model (dynamic or default)."""
    if _dynamic_config:
        return _dynamic_config["chairman"]
    return CHAIRMAN_MODEL


async def stage1_collect_responses(user_query: str) -> List[Dict[str, Any]]:
    """
    Stage 1: Collect individual responses from all council models.

    Args:
        user_query: The user's question

    Returns:
        List of dicts with 'model' and 'response' keys
    """
    messages = [{"role": "user", "content": user_query}]

    # Query all models in parallel
    council_models = get_council_models()
    responses = await query_models_parallel(council_models, messages)

    # Format results
    stage1_results = []
    for model, response in responses.items():
        if response is not None:  # Only include successful responses
            stage1_results.append({
                "model": model,
                "response": response.get('content', '')
            })

    return stage1_results


async def stage2_collect_rankings(
    user_query: str,
    stage1_results: List[Dict[str, Any]]
) -> Tuple[List[Dict[str, Any]], Dict[str, str]]:
    """
    Stage 2: Each model ranks the anonymized responses.

    Args:
        user_query: The original user query
        stage1_results: Results from Stage 1

    Returns:
        Tuple of (rankings list, label_to_model mapping)
    """
    # Create anonymized labels for responses (Response A, Response B, etc.)
    labels = [chr(65 + i) for i in range(len(stage1_results))]  # A, B, C, ...

    # Create mapping from label to model name
    label_to_model = {
        f"Response {label}": result['model']
        for label, result in zip(labels, stage1_results)
    }

    # Build the ranking prompt
    responses_text = "\n\n".join([
        f"Response {label}:\n{result['response']}"
        for label, result in zip(labels, stage1_results)
    ])

    ranking_prompt = f"""You are evaluating different responses to the following question:

Question: {user_query}

Here are the responses from different models (anonymized):

{responses_text}

Your task:
1. First, evaluate each response individually. For each response, explain what it does well and what it does poorly.
2. Then, at the very end of your response, provide a final ranking.

IMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:
- Start with the line "FINAL RANKING:" (all caps, with colon)
- Then list the responses from best to worst as a numbered list
- Each line should be: number, period, space, then ONLY the response label (e.g., "1. Response A")
- Do not add any other text or explanations in the ranking section

Example of the correct format for your ENTIRE response:

Response A provides good detail on X but misses Y...
Response B is accurate but lacks depth on Z...
Response C offers the most comprehensive answer...

FINAL RANKING:
1. Response C
2. Response A
3. Response B

Now provide your evaluation and ranking:"""

    messages = [{"role": "user", "content": ranking_prompt}]

    # Get rankings from all council models in parallel
    council_models = get_council_models()
    responses = await query_models_parallel(council_models, messages)

    # Format results
    stage2_results = []
    for model, response in responses.items():
        if response is not None:
            full_text = response.get('content', '')
            parsed = parse_ranking_from_text(full_text)
            stage2_results.append({
                "model": model,
                "ranking": full_text,
                "parsed_ranking": parsed
            })

    return stage2_results, label_to_model


async def stage2_5_debate(
    user_query: str,
    stage1_results: List[Dict[str, Any]],
    stage2_results: List[Dict[str, Any]],
    num_tours: int = 2
) -> List[Dict[str, Any]]:
    """
    Stage 2.5: Debate phase where LLMs can react to each other's responses and evaluations.
    
    Args:
        user_query: The original user query
        stage1_results: Individual model responses from Stage 1
        stage2_results: Rankings from Stage 2
        num_tours: Number of debate rounds (default: 2)
    
    Returns:
        List of debate rounds, each containing responses from all models
    """
    debate_rounds = []
    
    # Build context for debate
    stage1_text = "\n\n".join([
        f"**{result['model']}** said:\n{result['response']}"
        for result in stage1_results
    ])
    
    stage2_text = "\n\n".join([
        f"**{result['model']}** evaluated and ranked the responses:\n{result['ranking']}"
        for result in stage2_results
    ])
    
    # Track debate history for each model
    debate_history = {result['model']: [] for result in stage1_results}
    
    for tour_num in range(1, num_tours + 1):
        print(f"Stage 2.5: Starting debate tour {tour_num}/{num_tours}")
        
        # Build debate prompt for this tour
        if tour_num == 1:
            # First tour: initial reactions
            debate_prompt = f"""You are participating in a debate about the following question:

**Original Question:** {user_query}

**Initial Responses (Stage 1):**
{stage1_text}

**Peer Evaluations (Stage 2):**
{stage2_text}

**Your Task:**
This is the first round of debate. You can:
- Defend or clarify your initial response
- Respond to criticisms from the evaluations
- Point out strengths or weaknesses in other responses
- Refine or expand on your position based on the discussion

Provide your contribution to this debate round:"""
        else:
            # Subsequent tours: reactions to previous debate
            previous_tour_text = "\n\n".join([
                f"**{resp['model']}** said:\n{resp['response']}"
                for resp in debate_rounds[-1]['responses']
            ])
            
            debate_prompt = f"""You are participating in a debate about the following question:

**Original Question:** {user_query}

**Initial Responses (Stage 1):**
{stage1_text}

**Peer Evaluations (Stage 2):**
{stage2_text}

**Previous Debate Round {tour_num - 1}:**
{previous_tour_text}

**Your Task:**
This is round {tour_num} of the debate. You can:
- Respond to points raised by other models in the previous round
- Defend your position against new criticisms
- Acknowledge valid points from others
- Refine your argument further

Provide your contribution to this debate round:"""
        
        messages = [{"role": "user", "content": debate_prompt}]
        
        # Get debate responses from all models in parallel
        debate_responses = await query_models_parallel(
            [result['model'] for result in stage1_results],
            messages
        )
        
        # Format results for this tour
        tour_responses = []
        for result in stage1_results:
            model = result['model']
            response = debate_responses.get(model)
            
            if response is not None:
                content = response.get('content', '')
                tour_responses.append({
                    "model": model,
                    "response": content
                })
                # Track history for this model
                debate_history[model].append(content)
            else:
                # If model failed, use a placeholder or skip
                print(f"Warning: {model} failed to respond in debate tour {tour_num}")
        
        if tour_responses:
            debate_rounds.append({
                "tour": tour_num,
                "responses": tour_responses
            })
    
    return debate_rounds


async def stage3_synthesize_final(
    user_query: str,
    stage1_results: List[Dict[str, Any]],
    stage2_results: List[Dict[str, Any]],
    stage2_5_debate: List[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Stage 3: Chairman synthesizes final response.

    Args:
        user_query: The original user query
        stage1_results: Individual model responses from Stage 1
        stage2_results: Rankings from Stage 2

    Returns:
        Dict with 'model' and 'response' keys
    """
    # Build comprehensive context for chairman
    stage1_text = "\n\n".join([
        f"Model: {result['model']}\nResponse: {result['response']}"
        for result in stage1_results
    ])

    stage2_text = "\n\n".join([
        f"Model: {result['model']}\nRanking: {result['ranking']}"
        for result in stage2_results
    ])
    
    # Include debate if available
    debate_text = ""
    if stage2_5_debate and len(stage2_5_debate) > 0:
        debate_rounds_text = []
        for round_data in stage2_5_debate:
            round_num = round_data['tour']
            round_responses = "\n\n".join([
                f"**{resp['model']}**: {resp['response']}"
                for resp in round_data['responses']
            ])
            debate_rounds_text.append(f"Round {round_num}:\n{round_responses}")
        debate_text = "\n\n".join(debate_rounds_text)

    chairman_prompt = f"""You are the Chairman of an LLM Council. Multiple AI models have provided responses to a user's question, ranked each other's responses, and engaged in a debate.

Original Question: {user_query}

STAGE 1 - Individual Responses:
{stage1_text}

STAGE 2 - Peer Rankings:
{stage2_text}"""
    
    if debate_text:
        chairman_prompt += f"""

STAGE 2.5 - Debate:
{debate_text}"""
    
    chairman_prompt += """

Your task as Chairman is to synthesize all of this information into a single, comprehensive, accurate answer to the user's original question. Consider:
- The individual responses and their insights
- The peer rankings and what they reveal about response quality
- The debate discussions and how positions evolved
- Any patterns of agreement or disagreement
- The final consensus or key disagreements

Provide a clear, well-reasoned final answer that represents the council's collective wisdom:"""

    messages = [{"role": "user", "content": chairman_prompt}]

    # Log prompt size for debugging
    chairman_model = get_chairman_model()
    prompt_size = len(chairman_prompt)
    print(f"Stage 3: Querying {chairman_model} with prompt size: {prompt_size} characters")
    
    # If prompt is too long, truncate stage1 and stage2 text
    MAX_PROMPT_SIZE = 100000  # ~100k chars should be safe for most models
    if prompt_size > MAX_PROMPT_SIZE:
        print(f"Warning: Prompt too long ({prompt_size} chars), truncating...")
        # Truncate each response to max length
        max_per_response = MAX_PROMPT_SIZE // (len(stage1_results) + len(stage2_results) + 10)
        stage1_text = "\n\n".join([
            f"Model: {result['model']}\nResponse: {result['response'][:max_per_response]}..."
            for result in stage1_results
        ])
        stage2_text = "\n\n".join([
            f"Model: {result['model']}\nRanking: {result['ranking'][:max_per_response]}..."
            for result in stage2_results
        ])
        chairman_prompt = f"""You are the Chairman of an LLM Council. Multiple AI models have provided responses to a user's question, and then ranked each other's responses.

Original Question: {user_query}

STAGE 1 - Individual Responses:
{stage1_text}

STAGE 2 - Peer Rankings:
{stage2_text}

Your task as Chairman is to synthesize all of this information into a single, comprehensive, accurate answer to the user's original question. Consider:
- The individual responses and their insights
- The peer rankings and what they reveal about response quality
- Any patterns of agreement or disagreement

Provide a clear, well-reasoned final answer that represents the council's collective wisdom:"""
        messages = [{"role": "user", "content": chairman_prompt}]

    # Query the chairman model with longer timeout (synthesis can take time)
    chairman_model = get_chairman_model()
    response = await query_model(chairman_model, messages, timeout=180.0)

    if response is None:
        # Fallback: Use the best response from stage 1 based on aggregate rankings
        print(f"Stage 3: Failed to get response from {chairman_model}, using fallback")
        
        # Create label_to_model mapping for fallback
        labels = [chr(65 + i) for i in range(len(stage1_results))]
        label_to_model = {
            f"Response {label}": result['model']
            for label, result in zip(labels, stage1_results)
        }
        
        # Calculate aggregate rankings
        aggregate_rankings = calculate_aggregate_rankings(stage2_results, label_to_model)
        
        # Use the top-ranked response as fallback
        if aggregate_rankings and len(aggregate_rankings) > 0:
            best_model = aggregate_rankings[0]['model']
            best_response = next(
                (r['response'] for r in stage1_results if r['model'] == best_model),
                stage1_results[0]['response'] if stage1_results else "No response available"
            )
            return {
                "model": f"{chairman_model} (fallback: {best_model})",
                "response": f"[Note: Chairman synthesis failed, using top-ranked response from {best_model}]\n\n{best_response}"
            }
        elif stage1_results:
            # If no rankings available, use first response
            return {
                "model": f"{chairman_model} (fallback)",
                "response": f"[Note: Chairman synthesis failed, using first available response]\n\n{stage1_results[0]['response']}"
            }
        else:
            return {
                "model": chairman_model,
                "response": "Error: Unable to generate final synthesis."
            }

    return {
        "model": chairman_model,
        "response": response.get('content', '')
    }


def parse_ranking_from_text(ranking_text: str) -> List[str]:
    """
    Parse the FINAL RANKING section from the model's response.

    Args:
        ranking_text: The full text response from the model

    Returns:
        List of response labels in ranked order
    """
    import re

    # Look for "FINAL RANKING:" section
    if "FINAL RANKING:" in ranking_text:
        # Extract everything after "FINAL RANKING:"
        parts = ranking_text.split("FINAL RANKING:")
        if len(parts) >= 2:
            ranking_section = parts[1]
            # Try to extract numbered list format (e.g., "1. Response A")
            # This pattern looks for: number, period, optional space, "Response X"
            numbered_matches = re.findall(r'\d+\.\s*Response [A-Z]', ranking_section)
            if numbered_matches:
                # Extract just the "Response X" part
                return [re.search(r'Response [A-Z]', m).group() for m in numbered_matches]

            # Fallback: Extract all "Response X" patterns in order
            matches = re.findall(r'Response [A-Z]', ranking_section)
            return matches

    # Fallback: try to find any "Response X" patterns in order
    matches = re.findall(r'Response [A-Z]', ranking_text)
    return matches


def calculate_aggregate_rankings(
    stage2_results: List[Dict[str, Any]],
    label_to_model: Dict[str, str]
) -> List[Dict[str, Any]]:
    """
    Calculate aggregate rankings across all models.

    Args:
        stage2_results: Rankings from each model
        label_to_model: Mapping from anonymous labels to model names

    Returns:
        List of dicts with model name and average rank, sorted best to worst
    """
    from collections import defaultdict

    # Track positions for each model
    model_positions = defaultdict(list)

    for ranking in stage2_results:
        ranking_text = ranking['ranking']

        # Parse the ranking from the structured format
        parsed_ranking = parse_ranking_from_text(ranking_text)

        for position, label in enumerate(parsed_ranking, start=1):
            if label in label_to_model:
                model_name = label_to_model[label]
                model_positions[model_name].append(position)

    # Calculate average position for each model
    aggregate = []
    for model, positions in model_positions.items():
        if positions:
            avg_rank = sum(positions) / len(positions)
            aggregate.append({
                "model": model,
                "average_rank": round(avg_rank, 2),
                "rankings_count": len(positions)
            })

    # Sort by average rank (lower is better)
    aggregate.sort(key=lambda x: x['average_rank'])

    return aggregate


async def generate_conversation_title(user_query: str) -> str:
    """
    Generate a short title for a conversation based on the first user message.

    Args:
        user_query: The first user message

    Returns:
        A short title (3-5 words)
    """
    title_prompt = f"""Generate a very short title (3-5 words maximum) that summarizes the following question.
The title should be concise and descriptive. Do not use quotes or punctuation in the title.

Question: {user_query}

Title:"""

    messages = [{"role": "user", "content": title_prompt}]

    # Use a lightweight model for title generation
    # Try to use a small model from current provider, fallback to first available
    council_models = get_council_models()
    title_model = council_models[0] if council_models else "llama3"
    response = await query_model(title_model, messages, timeout=30.0)

    if response is None:
        # Fallback to a generic title
        return "New Conversation"

    title = response.get('content', 'New Conversation').strip()

    # Clean up the title - remove quotes, limit length
    title = title.strip('"\'')

    # Truncate if too long
    if len(title) > 50:
        title = title[:47] + "..."

    return title


async def run_full_council(user_query: str) -> Tuple[List, List, Dict, Dict]:
    """
    Run the complete 3-stage council process.

    Args:
        user_query: The user's question

    Returns:
        Tuple of (stage1_results, stage2_results, stage2_5_debate, stage3_result, metadata)
    """
    # Stage 1: Collect individual responses
    stage1_results = await stage1_collect_responses(user_query)

    # If no models responded successfully, return error
    if not stage1_results:
        return [], [], {
            "model": "error",
            "response": "All models failed to respond. Please try again."
        }, {}

    # Stage 2: Collect rankings (only if we have responses)
    stage2_results = []
    label_to_model = {}
    if stage1_results:
        stage2_results, label_to_model = await stage2_collect_rankings(user_query, stage1_results)

    # Calculate aggregate rankings
    aggregate_rankings = calculate_aggregate_rankings(stage2_results, label_to_model)

    # Stage 2.5: Debate (optional, can be disabled)
    debate_rounds = await stage2_5_debate(
        user_query,
        stage1_results,
        stage2_results,
        num_tours=2  # 2 rounds of debate
    )

    # Stage 3: Synthesize final answer
    stage3_result = await stage3_synthesize_final(
        user_query,
        stage1_results,
        stage2_results,
        debate_rounds
    )

    # Prepare metadata
    metadata = {
        "label_to_model": label_to_model,
        "aggregate_rankings": aggregate_rankings
    }

    return stage1_results, stage2_results, debate_rounds, stage3_result, metadata
