# Roadmap: Migration vers Ollama

## Phase 1: Pr√©paration (30 min)

### 1.1 Installation d'Ollama
- [ ] Installer Ollama: https://ollama.ai
- [ ] V√©rifier l'installation: `ollama --version`
- [ ] D√©marrer le service Ollama

### 1.2 T√©l√©chargement des mod√®les
- [ ] T√©l√©charger les mod√®les choisis:
  ```bash
  ollama pull llama3
  ollama pull mistral
  ollama pull codellama
  ollama pull neural-chat
  ```
- [ ] V√©rifier la liste: `ollama list`
- [ ] Tester un mod√®le: `ollama run llama3 "Hello"`

### 1.3 V√©rification de l'API
- [ ] Tester l'API: `curl http://localhost:11434/api/tags`
- [ ] Tester un appel chat: 
  ```bash
  curl http://localhost:11434/api/chat -d '{
    "model": "llama3",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
  ```

## Phase 2: D√©veloppement (2-3 heures)

### 2.1 Cr√©ation du client Ollama
- [ ] Cr√©er `backend/ollama.py`
- [ ] Impl√©menter `query_model()` avec format Ollama
- [ ] Impl√©menter `query_models_parallel()`
- [ ] G√©rer les erreurs sp√©cifiques (service non d√©marr√©, mod√®le inexistant)
- [ ] Ajouter logging appropri√©

**Checklist technique:**
- [ ] URL: `http://localhost:11434/api/chat`
- [ ] Format requ√™te: `{"model": str, "messages": List[Dict], "stream": False}`
- [ ] Format r√©ponse: Extraire `message.content`
- [ ] Gestion timeout
- [ ] Gestion erreurs HTTP
- [ ] Interface identique √† `openrouter.py`

### 2.2 Modification de la configuration
- [ ] Modifier `backend/config.py`:
  - [ ] Supprimer `OPENROUTER_API_KEY`
  - [ ] Supprimer `OPENROUTER_API_URL`
  - [ ] Ajouter `OLLAMA_API_URL`
  - [ ] Mettre √† jour `COUNCIL_MODELS` avec noms Ollama
  - [ ] Mettre √† jour `CHAIRMAN_MODEL`
- [ ] Mettre √† jour `.env` (supprimer cl√© API)

### 2.3 Mise √† jour des imports
- [ ] Modifier `backend/council.py`: changer import `openrouter` ‚Üí `ollama`
- [ ] V√©rifier qu'aucun autre fichier n'importe `openrouter`

### 2.4 Gestion de la disponibilit√©
- [ ] Ajouter fonction `check_ollama_available()` dans `ollama.py`
- [ ] Appeler au d√©marrage du backend (optionnel, avec warning si indisponible)

## Phase 3: Tests (1-2 heures)

### 3.1 Tests unitaires
- [ ] Tester `query_model()` avec un mod√®le valide
- [ ] Tester `query_model()` avec un mod√®le inexistant
- [ ] Tester `query_model()` avec Ollama non d√©marr√©
- [ ] Tester `query_models_parallel()` avec plusieurs mod√®les
- [ ] Tester avec des prompts longs

### 3.2 Tests d'int√©gration
- [ ] Tester Stage 1 (r√©ponses individuelles)
- [ ] Tester Stage 2 (rankings)
- [ ] Tester Stage 2.5 (d√©bat)
- [ ] Tester Stage 3 (synth√®se)
- [ ] Tester le flux complet end-to-end

### 3.3 Tests de performance
- [ ] Mesurer le temps de r√©ponse (comparer avec OpenRouter si possible)
- [ ] Tester avec diff√©rents mod√®les
- [ ] V√©rifier la consommation m√©moire

### 3.4 Tests d'erreurs
- [ ] Mod√®le non disponible
- [ ] Ollama non d√©marr√©
- [ ] Timeout
- [ ] R√©ponse vide
- [ ] Erreur r√©seau

## Phase 4: Documentation (30 min)

### 4.1 Mise √† jour README
- [ ] Section "Setup" - instructions Ollama
- [ ] Section "Configuration" - nouveaux noms de mod√®les
- [ ] Section "Requirements" - ressources syst√®me recommand√©es
- [ ] Section "Troubleshooting" - probl√®mes courants Ollama

### 4.2 Documentation technique
- [ ] Commenter le code `ollama.py`
- [ ] Documenter les diff√©rences avec OpenRouter
- [ ] Ajouter exemples de configuration

## Phase 5: Optimisations (optionnel, 1-2 heures)

### 5.1 Performance
- [ ] Impl√©menter le streaming pour meilleur UX
- [ ] Cache des r√©ponses (si pertinent)
- [ ] Pool de connexions HTTP

### 5.2 Fonctionnalit√©s
- [ ] D√©tection automatique des mod√®les disponibles
- [ ] Fallback si un mod√®le n'est pas disponible
- [ ] Configuration dynamique des mod√®les via UI

### 5.3 Monitoring
- [ ] Logs de performance
- [ ] M√©triques de temps de r√©ponse
- [ ] Alertes si Ollama est indisponible

## Phase 6: D√©ploiement (30 min)

### 6.1 Pr√©paration
- [ ] V√©rifier que tous les mod√®les sont t√©l√©charg√©s
- [ ] Tester sur l'environnement cible
- [ ] V√©rifier les ressources syst√®me

### 6.2 Migration
- [ ] Backup de la configuration actuelle
- [ ] Appliquer les changements
- [ ] Red√©marrer le backend
- [ ] V√©rifier que tout fonctionne

### 6.3 Rollback plan
- [ ] Garder `openrouter.py` en backup
- [ ] Documenter comment revenir en arri√®re
- [ ] Tester le rollback

## Timeline estim√©e

| Phase | Dur√©e | Priorit√© |
|-------|-------|----------|
| Phase 1: Pr√©paration | 30 min | üî¥ Critique |
| Phase 2: D√©veloppement | 2-3h | üî¥ Critique |
| Phase 3: Tests | 1-2h | üî¥ Critique |
| Phase 4: Documentation | 30 min | üü° Important |
| Phase 5: Optimisations | 1-2h | üü¢ Optionnel |
| Phase 6: D√©ploiement | 30 min | üî¥ Critique |
| **TOTAL** | **5.5-8.5h** | |

## Crit√®res de succ√®s

‚úÖ Tous les stages fonctionnent avec Ollama
‚úÖ Performance acceptable (< 2x plus lent qu'OpenRouter)
‚úÖ Gestion d'erreurs robuste
‚úÖ Documentation √† jour
‚úÖ Tests passent tous

## Risques et mitigation

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| Performance trop lente | Moyenne | √âlev√© | Choisir mod√®les plus petits, optimiser |
| Mod√®les incompatibles | Faible | Moyen | Tester avant, avoir fallback |
| Ollama crash | Faible | √âlev√© | Gestion d'erreurs, red√©marrage auto |
| RAM insuffisante | Moyenne | √âlev√© | Recommander mod√®les selon RAM |

## Notes importantes

1. **Compatibilit√©**: Garder l'interface identique permet un rollback facile
2. **Mod√®les**: Tester avec plusieurs mod√®les pour trouver le meilleur √©quilibre
3. **Performance**: Les mod√®les 7B-13B sont un bon compromis qualit√©/vitesse
4. **GPU**: Si disponible, utiliser GPU pour meilleures performances

## Prochaines √©tapes

1. D√©marrer par la Phase 1 (installation Ollama)
2. Tester manuellement quelques appels API
3. Impl√©menter `ollama.py` en suivant l'interface existante
4. Tester progressivement chaque stage
5. Documenter les diff√©rences observ√©es

